{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Natural language processing\n",
    "Â© ExploreAI Academy\n",
    "\n",
    "In this exercise, we will perform text preprocessing tasks such as converting to lowercase, removing punctuation, creating a bag-of-words, and applying stemming and lemmatization techniques in order to analyse text data to gain some insights.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this exercise, you should be able to:\n",
    "* Implement text preprocessing techniques such as converting to lowercase and removing punctuation.\n",
    "* Apply stemming and lemmatization techniques to extract the root forms of words.\n",
    "* Create a bag-of-words representation to quantify the occurrence of words in text.\n",
    "* Calculate statistics such as the number of stop words, unique words, and word frequencies in text data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import TreebankWordTokenizer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import urllib\n",
    "from text_processor_utils import read_online_text_data, read_local_text_data, remove_punctuations\n",
    "from text_processor_utils import bag_of_words_count\n",
    "import pandas as pd\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this notebook is text from the book \"Alice's Adventures in Wonderland\" by Lewis Carroll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef print_some_url():\\n    with urllib.request.urlopen('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint//alice_in_wonderland.txt') as f:\\n        return f.read().decode('ISO-8859-1')\\n\\ndata = print_some_url()\\nprint(data[:863])\\n\\n\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I will only execute the code block below once. \n",
    "# read in the data--- I have done this into one whole function to handle repetition \n",
    "\"\"\"\n",
    "def print_some_url():\n",
    "    with urllib.request.urlopen('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint//alice_in_wonderland.txt') as f:\n",
    "        return f.read().decode('ISO-8859-1')\n",
    "\n",
    "data = print_some_url()\n",
    "print(data[:863])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nurl = 'https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint//alice_in_wonderland.txt'\\ndataset = read_online_text_data(url)\\n\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the data\n",
    "\"\"\"\n",
    "url = 'https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint//alice_in_wonderland.txt'\n",
    "dataset = read_online_text_data(url)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_path = r\"C:\\Users\\Allan\\Desktop\\Natural Language Processing\\txt_data.txt\"\n",
    "data = read_local_text_data(file_path= file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's Adventures in Wonderland\n",
      "\n",
      "                ALICE'S ADVENTURES IN WONDERLAND\n",
      "\n",
      "                          Lewis Carroll\n",
      "\n",
      "               THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            CHAPTER I\n",
      "\n",
      "                      Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "  Alice was beginning to get very tired of sitting by her sister\n",
      "on the bank, and of having nothing to do:  once or twice she had\n",
      "peeped into the book her sister was reading, but it had no\n",
      "pictures or conversations in it, `and what is the use of a book,'\n",
      "thought Alice `without pictures or conversation?'\n",
      "\n",
      "  So she was considering in her own mind (as well as she could,\n",
      "for the hot day made her feel very sleepy and stupid), whether\n",
      "the pleasure of making a daisy-chain would be worth the trouble\n",
      "of getting up and picking the daisies, when suddenly a White\n",
      "Rabbit with pink eyes ran close by her.\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(data[:863])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148574"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "We will first start by providing you with the functions required to remove punctuation, create a bag-of-words, and define a stemmer, tokeniser, and lemmatizer. Once you apply the functions to preprocess the data, you will be asked to perform some calculations and analysis in the exercise questions below.\n",
    "\n",
    "\n",
    "**Convert to lowercase and remove punctuation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alices adventures in wonderland\n",
      "\n",
      "                alices adventures in wonderland\n",
      "\n",
      "                          lewis carroll\n",
      "\n",
      "               the millennium fulcrum edition 30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            chapter i\n",
      "\n",
      "                      down the rabbithole\n",
      "\n",
      "\n",
      "  alice was beginning to get very tired of sitting by her sister\n",
      "on the bank and of having nothing to do  once or twice she had\n",
      "peeped into the book her sister was reading but it had no\n",
      "pictures or conversations in it and what is the use of a book\n",
      "thought alice without pictures or conversation\n",
      "\n",
      "  so she was considering in her own mind as well as she could\n",
      "for the hot day made her feel very sleepy and stupid whether\n",
      "the pleasure of making a daisychain would be worth the trouble\n",
      "of getting up and picking the daisies when suddenly a white\n",
      "rabbit with pink eyes ran close by her\n",
      "\n",
      "  there was nothing so\n"
     ]
    }
   ],
   "source": [
    "#Function to remove punctuation\n",
    "# I have my own custom function in the text_processor fle\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    words = words.lower()\n",
    "    return ''.join([x for x in words if x not in string.punctuation])\n",
    "\n",
    "\n",
    "_data_ =  remove_punctuation(data)\n",
    "print(_data_[:863])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the remove_punctuation function to the data\n",
    "data = remove_punctuations(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alices adventures in wonderland\n",
      "\n",
      "                alices adventures in wonderland\n",
      "\n",
      "                          lewis carroll\n",
      "\n",
      "               the millennium fulcrum edition 30\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            chapter i\n",
      "\n",
      "                      down the rabbithole\n",
      "\n",
      "\n",
      "  alice was beginning to get very tired of sitting by her sister\n",
      "on the bank and of having nothing to do  once or twice she had\n",
      "peeped into the book her sister was reading but it had no\n",
      "pictures or conversations in it and what is the use of a book\n",
      "thought alice without pictures or conversation\n",
      "\n",
      "  so she was considering in her own mind as well as she could\n",
      "for the hot day made her feel very sleepy and stupid whether\n",
      "the pleasure of making a daisychain would be worth the trouble\n",
      "of getting up and picking the daisies when suddenly a white\n",
      "rabbit with pink eyes ran close by her\n",
      "\n",
      "  there was nothing so\n"
     ]
    }
   ],
   "source": [
    "print(data[:863])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bag-of-words and assign our stemmer and lemmatizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "data_tokens = tokenizer.tokenize(text= data)\n",
    "#Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "#Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alices', 'adventures', 'in', 'wonderland', 'alices', 'adventures', 'in', 'wonderland', 'lewis', 'carroll']\n"
     ]
    }
   ],
   "source": [
    "print(data_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef bag_of_words_count(words:list, word_dict= {}):\\n    for word in words:\\n        if word in word_dict.keys():\\n            word_dict[word] += 1\\n        else:\\n            word_dict[word]= 1\\n    return word_dict\\n\\n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a bag of words--- \n",
    "# I also have included thie funtion in the text processor utils\n",
    "\"\"\"\n",
    "def bag_of_words_count(words:list, word_dict= {}):\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word]= 1\n",
    "    return word_dict\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "tokens_less_stopwords = [word for word in data_tokens if word not in stopwords.words('english')]\n",
    "#Then create a bag of words\n",
    "bag_of_words = bag_of_words_count(words = tokens, word_dict= {})\n",
    "#bag_of_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay special attention to what these functions return and how the subsequent texts and lists look."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the stemmer and lemmatizer functions (defined in the cells above) from the relevant library to write a function that finds the stem and lemma of the nth word in the token list.\n",
    "\n",
    "_**Function specifications:**_\n",
    "* Should take a `list` as input and return a  `dict` type as output.\n",
    "* The dictionary should have the keys **'original',  'stem', and 'lemma'** with the corresponding values being the nth word transformed in that way.\n",
    "\n",
    "**Example result:**\n",
    "\n",
    "`{'original': 'daisies', \n",
    "'stem': 'daisi', \n",
    "'lemma': 'daisy'}`\n",
    "\n",
    "Use your function to find the 120th word in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_less_stopwords[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'daisies', 'stem': 'daisi', 'lemma': 'daisy'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "def find_stemmer_and_lemma(words:list, n:int):\n",
    "    out_put_dict = { }\n",
    "    target_word = words[n-1]\n",
    "    stemmed = stemmer.stem(target_word)\n",
    "    lemmatized = lemmatizer.lemmatize(target_word)\n",
    "    out_put_dict['original'] = target_word\n",
    "    out_put_dict['stem'] = stemmed\n",
    "    out_put_dict['lemma'] = lemmatized\n",
    "    return out_put_dict\n",
    "\n",
    "\n",
    "find_stemmer_and_lemma(data_tokens, 120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that calculates the number of stop words that are in the text in total, including repetitions.   \n",
    "\n",
    "_Hint:_ You can use the nltk stopwords dictionary. \n",
    "\n",
    "_**Function specifications:**_\n",
    "* Function should take a `list` as input. \n",
    "* The number of stop words should be returned as an `int`. \n",
    "\n",
    "Use your function to calculate the total number of stop words in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "stopwords_list = stopwords.words('english')\n",
    "def calculate_number_of_stopwords(word_list: list):\n",
    "    total_stopwords = len([n for n in word_list if n in stopwords_list])\n",
    "    return total_stopwords\n",
    "\n",
    "\n",
    "calculate_number_of_stopwords(tokens_less_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Write a function that calculates the number of **unique** words in the text.\n",
    "\n",
    "_**Function specifications:**_\n",
    "* Function should take a `list` as input and return an `int`. \n",
    "\n",
    "\n",
    "Use your function to calculate the number of **unique** words in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "def get_number_of_unique_words(word_list: list):\n",
    "    unique_words = set(word_list)\n",
    "    total_number_of_unique_words = len(unique_words)\n",
    "    return total_number_of_unique_words\n",
    "\n",
    "get_number_of_unique_words(data_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that calculates the kth most frequently occurring word in the bag-of-words.\n",
    "\n",
    "_**Function specifications:**_\n",
    "* Function should take a `dict` and an `int` k as input.\n",
    "* Function should return the kth most common word as a `str`.\n",
    "\n",
    "_Hint:_ bag_of_words already does not include stop words.\n",
    "\n",
    "**Example input:**\n",
    "```python\n",
    "most_common_word(bag = {'apple': 30, 'orange': 12, 'pear': 50, 'banana': 12}, 2)\n",
    "\n",
    ">>> 'apple'\n",
    "```\n",
    "\n",
    "\n",
    "Use the function to calculate the 3rd most frequently occurring word in the bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switched =[(value, key) for key, value in bag_of_words.items()]\n",
    "switched_list = sorted(switched, reverse= True)\n",
    "switched_list[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "def kth_frequency(bag:list, k):\n",
    "    switched =[(value, key) for key, value in bag.items()]\n",
    "    switched_list = sorted(switched, reverse= True)\n",
    "    return(switched_list[k-1][1])\n",
    "\n",
    "kth_frequency(bag_of_words, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Write a function that calculates the number of words that appear n times in the text.\n",
    "\n",
    "_**Function specifications:**_\n",
    "* Input is taken as a `dict` and an `int` n, where n is the number of times the word appears in the text.\n",
    "* Count the number of words that appear n times in the text.\n",
    "* Output should be the count as an `int`.\n",
    "\n",
    "**Example input:** \n",
    "```python\n",
    "word_frequency_count(bag = {'apple': 30, 'orange': 12, 'pear': 50, 'banana': 12}, 12)\n",
    "\n",
    ">>> 2\n",
    "```\n",
    "\n",
    "Use the function to calculate the number of words that appear eight times in the bag-of-words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 8\n",
    "_words_ = []\n",
    "_words_ +=([word for word in bag_of_words.keys()if bag_of_words[word] == n])\n",
    "\n",
    "len(_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "def word_frequency_count(bag:dict, n):\n",
    "    _words_ = []\n",
    "    _words_ +=(word for word in bag.keys() if bag[word]==n)\n",
    "    return len(_words_)\n",
    "\n",
    "\n",
    "word_frequency_count(bag_of_words, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_roots(token_list, n):\n",
    "    \n",
    "    root_dict = {}\n",
    "    word = token_list[n-1]\n",
    "    root_dict['original'] = word\n",
    "    root_dict[\"stem\"] = stemmer.stem(word)\n",
    "    root_dict[\"lemma\"] = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    return root_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'daisies', 'stem': 'daisi', 'lemma': 'daisy'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_roots(tokens, 120) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(token_list):\n",
    "    STOPwords = [word for word in token_list if word in stopwords.words(\"english\")]\n",
    "    return len(STOPwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13774"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_stopwords(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(token_list):\n",
    "    return len(set(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The same result can be achieved by using the `len()` function on the `bag_of_words_count()` function to calculate the number of unique words in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bag_of_words_count(tokens,{}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_word(bag, k):\n",
    "    switch = [(value, key) for key, value in bag.items()]\n",
    "    switch = sorted(switch)\n",
    "    return switch[-k][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_word(bag_of_words, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_count(bag, n):\n",
    "    total = sum(1 for value in bag.values() if value == n)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_count(bag_of_words, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
